\documentclass[12pt]{article}
\usepackage{amsmath,setspace,graphicx,amssymb,amsthm, float}
\title{\textbf{MATH1104 Personal Lecture Notes}}
\author{Thomas Piercy}
\date{\today{}}

\newcommand{\rational}{\in\mathbb{Q}}
\newcommand{\irrational}{\in\mathbb{R}\backslash\mathbb{Q}}
\newcommand{\integer}{\in\mathbb{Z}}

\begin{document}
\maketitle
\hrulefill
\setstretch{1.5}
\section{Proofs and Definitions}
\subsection*{Definitions}
Often we need to clearly define things in mathematics so that we can standardise our work or at least better communicate our work to others. Proofs rely on definitions because they tell us what we can know for certain before starting and remove any ambiguity over symbols or variables. Typically, a definition will give a name to a type of object that satisfies some properties: An example of this are the prime numbers ("prime numbers" are "an integer" \(p\) such that "\(p\) greater than \(2\) and not divisible except by \(1\) and \(p\)").

In a definition, the word or phrase being defined is often bolded or underlined for clarity.
\subsection*{A little about logic}
Mathematical statements can be true or false and we can represent this with a variable. We can chain these statements together with operators and evaluate if the result is still true/false however its worth bearing in mind that In Mathematics we always use the inclusive or, to mean A or B or Both.

We can also chain statements using implication, this is asserting that some statement Q is true when another statement P is true, an overview of implication notation is given below:
\begin{table}[h]
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Notation} & \textbf{Example} \\ \hline
       \(P\) implies \(Q\) & \(n^3<0\) implies \(n<0\) \\ \hline
        If \(P\), then \(Q\) & If \(a \geq 4\), then \(2a + 1 \geq 7\) \\ \hline
       \(P \implies Q\) & \(p\) is prime \(\implies p \geq 2\) \\ \hline
        \(Q \impliedby P\) & 2 \(\nmid p \impliedby p\) is prime \(> 2\) \\ \hline
        \(Q\) if \(P\) & \(2\) roots if \(b^2-4ac>0\) \\ \hline
        \(P\) only if \(Q\) & Stationary point only if \(\frac{\text{d}y}{\text{d}x}=0\) \\ \hline      
        \(R \iff S\) & \(x\) is positive \(\iff x>0\) \\ \hline
        \(R\) iff \(S\) & \(x\) is non-positive iff \(x \leq 0\) \\ \hline
    \end{tabular}
\end{table}
\subsection*{Some notation convention}
There is a convention that definitions only write "if", when in reality definitions are always "if and only if" or "iff". We also need to be particular when discussing the sign of a number:
\[x = \begin{cases}
x\text{ is positive},\quad x>0 \\
x\text{ is non-negative},\quad x\geq0 \\
x\text{ is negative},\quad x<0 \\
x\text{ is non-positive},\quad x\leq0
\end{cases}\]
\subsection*{Proof}
It is impossible to prove most mathematical statements by picking examples and checking if they are valid. However, it is often possible to disprove a claim by finding a counter-example. Sometimes questions will ask for a proof or a counter-example - in this case it is up to you to decide if you think the statement is true and should try to find a proof, or false and try to find an example.
\subsubsection*{Direct proof}
Direct proof works off of the notation that we assume all conditions and hypotheses are true and we then make deductions based off of known theorems and definitions to achieve the desired result.

\textbf{Statement:} If \(n\) is an even integer, then \(n^2\) is a multiple of \(4\)

\textbf{Proof:}
Let \(n\) be an even integer, \(n=2k\). Hence:
\[n^2=(2k)^2\]
\[(2k)^2=4k^2\]
\\As we can see \(n^2\) is a multiple of \(4\), satisfying the proof
\[\qed\]

It is important that we don't use the statement that we are trying to prove in our proof itself as this can lead to backwards logic and many wrong deductions. We should only take what is given to us either as fact or as necessary setup for the proof.
\section{Sets}
Note that the section on sets has been dramatically reduced in size due to content overlaps with MATH1101.
\subsection*{Minimums and maximums}
We can write the elements of a finite set of real numbers in strictly increasing order so that if we have a set \(S\):
\[S=\{s_1,s_2,...,s_n\}\]
Then we can write:
\[s_1<s_2<\dots<s_n\]
The minimum and maximum elements are denoted \(\min S\) and \(\max S\) respectively.
\subsection*{Cardinality}
let \(n\in\mathbb{N}\) if set \(A\) has exactly \(n\) elements it is said to have cardinality \(n,\text{ or}\;\vert A\vert=n\)
\[\vert\emptyset\vert=0\]
A set is finite if its cardinality is 0 or \(n\in\mathbb{N}\). If a set is not finite then it is infinite.
\subsection*{Power set}
Let \(X\) be a set, the power set of \(X\) demoted \(\mathcal{P}(X)\) or \(2^X\) is the set of all subsets of \(X\). This means that the power set is a set of sets.
\[Y\in \mathcal{P}(X) \iff Y\subseteq X\]
if \(X\) has \(n\) distinct elements then there are \(2^n\) possible subsets hence
\[\vert\mathcal{P}(X)\vert=2^n\]
A special case with the empty set:
\[\vert\mathcal{P}(\emptyset)\vert=\vert\{\emptyset\}\vert=1\]
\subsection*{Partitions}
All of the sets in each partition are pairwise disjoint, and every element in the original set is in a partition. The sets in a partition are called the cells of the partition.
\section{Number Theory}
\subsection*{Divisors}
We say that m divides n if there is an integer multiple of m that equals n. We can also say m is a factor or divisor of n and that n is a multiple of m.
\[m\mid n=\frac{m}{n}\in\mathbb{Z}\]
\[m\nmid n=\frac{m}{n}\not\in\mathbb{Z}\]
We say that \(n\) is even if we have \(n=2k\) for some integer \(k\). Note that by this definition of even, 0 is an even number. The positive common divisors of \(m\) and \(n\) are the positive integers that divide both \(m\) and \(n\). This set is always finite and non-empty. The greatest common divisor is the maximum of this set.
\[\gcd (m,n)=\text{hcf}\,(m,n)=\max (\{a\in\mathbb{N}:a\mid m\}\cap\{b\in \mathbb{N}:b\mid n\})\]
\subsection*{Primes}
Primes are integers greater than two whose only divisors are itself and 1. 1 is not a prime by this definition. Any integer \(> 1\) and not prime is called a composite number because it can be written as a product of primes. The proof of this theorem is a proof by contradiction, in particular what is known as a minimal counterexample proof (we assume a smallest counter-example and then show there is one smaller). Two numbers are said to be coprime if \(\gcd(m,n)=1\)
\subsection*{Division algorithm for integers}
Let \(n\in\mathbb{Z}\) and \(m\in\mathbb{N}\):
\[\exists\,q,r \in \mathbb{Z}:n=qm+r,\; 0\leq r<m\]
\subsubsection*{The euclidean algorithm}
Let \(a\in \mathbb{Z}, b\in \mathbb{N}\) and let \(q\) and \(r\) be integers such that \(a=qb+r\). Then:
\[\gcd(a,b)=\gcd(b,r)\]
To prove this what we want to show is that \(d\) is a common divisor of \(a\) and \(b \iff d\) is a common divisor of \(b\) and \(r\).

\textbf{Proof LHS\(\implies\)RHS:}

Assume \(d\mid a\) and \(d \mid b\). Then:
\[a=kd,\,b=ld\text{ for some }k,l\in\mathbb{Z}\]
\[r=a-qb=kd-qld=d(k-ql)\]
\[\text{Thus},\;d\mid r\]

\textbf{Proof RHS\(\implies\)LHS:}

Assume \(d\mid b\) and \(d \mid r\). Then:
\[b=k'd,\,r=l'd\text{ for some }k',l'\in\mathbb{Z}\]
\[a=qb+r=qk'd+l'd=d(qk'+l')\]
\[\text{Thus},\;d\mid a\]
\[\qed\]
\subsubsection*{Example}
Calculate \(\gcd(1071,378)\):
\[1071=2\cdot378+315\]
Hence
\[\gcd(1071,378)=\gcd(378,315)\]
\[378=1\cdot315+63\]
\[\gcd(1071,378)=\gcd(315,63)\]
\[315=5\cdot63+0\]
Since \(63 \mid 315\), we have:
\[\gcd(1071,378)=63\]
\subsection*{B\'ezouts lemma}
Let \(a,b\in\mathbb{Z}\). There exists \(k,l\in\mathbb{Z}\) such that \(\gcd(a,b)=ka+lb\)
\subsubsection*{Example}
\[\gcd(432,126)\]
\begin{equation}
    432=3\cdot126+54
\end{equation}
\[\gcd(432,126)=\gcd(126,54)\]
\begin{equation}
    126=2\cdot54+18
\end{equation}
\[\gcd(432,126)=\gcd(126,54)=\gcd(54,18)\]
\begin{equation}
    54=3\cdot18
\end{equation}
\[\therefore \gcd(432,126)=18\]
Going the other way with B\'ezouts lemma we get:
\[18=k\cdot432+l\cdot126\]
\[18=126-2\cdot54\]
\[18=126-2\cdot(432-3\cdot126)\]
\[18=126-2\cdot432+6\cdot126\]
\[18=7\cdot126-2\cdot432\]
Hence:
\[k=-2,\,l=7\]
\subsection*{Prime factors}
The fundamental theorem of arithmetic says that every number has a unique prime factorisation.

\textbf{Proof:}
Suppose the theorem is not true, then there must be a smallest integer \(n\geq 2\) for which the theorem fails.

Then \(n\) has at least 2 prime factorisations:
\[n = p_1^{k_1}p_2^{k_2}p_3^{k_3}... = q_1^{j_1}q_2^{j_2}q_3^{j_3}...\]
where the two lists of primes \(p_1 ... p_n\) and \(q_1 ... q_m\) are different or at least one element is raised to a different power. 
\\
Note that we have \(p_1 \mid n\) but since \(n = q_1^{j_1}...\) then we have \(p\mid q_i\) for some \(i\). This means that \(p_1=q_i\) since both are prime and greater than 1.
\\
Now consider 
\(\hat{n}=\frac{n}{p_1}=\frac{n}{q_i}\). We have that \(\hat{n}<n\) whilst also having two distinct prime factorisations
\\
This contradicts our choice of \(n\) being the smallest possible integer for which the theorem fails and therefore the theorem is true for all \(n\geq 2\)
\subsection*{Linear Diophantine equations}
A Diophantine equation is one where all constants and unknowns are integers. A linear Diophantine equation is a Diophantine equation of order 1:
\[ax+by=c\]
One can note that solutions to this equation only exist when \(\gcd(a,b) \mid c\). Next we should be aware that if one solution exists then multiple exist because
\[a(x+b)+b(y-a)=c\]
is also a valid solution. In fact \((x+nb)\text{ and }(y-na)\) are all solutions for \(n\in\mathbb{Z}\). This naturally raises the question, can we find all solutions of the equation? To start, it can be helpful to consider the homogenous equation:
\[au+bv=0,\,u,v\in\mathbb{Z}\]
Suppose we have a particular solution to our equation, then a new solution is given by:
\[x=x_P+u,\,y=y_p+v\]
This tells us that given one solution to our equation and the solutions for \(u,v\) we can find all solutions to our equation.
\[u=\frac{bn}{\gcd(a,b)} \text{ and }v=\frac{-an}{\gcd(a,b)},\; n\in\mathbb{Z}\]
Putting everything together gives us that:
\[x=x_P+\frac{bn}{\gcd(a,b)},\,y=y_p-\frac{an}{\gcd(a,b)}\]
Provided one solution from:
\[\gcd(a,b)\mid c\implies x_p=\frac{ck}{\gcd(a,b)},y_p=\frac{cl}{\gcd(a,b)}\]
\subsubsection*{Example}
\[42x-112y=70\]
\[\gcd(42,112)=14\]
\[14\mid70 \text{ so the equation is solvable}\]
\[x_p=\frac{70k}{14}, y_p=\frac{70l}{14}\]
Applying B\'ezouts identity:
\[14=42k-112l\implies k=3,\,l=1\]
Thus
\[x_p=15,\,y_p=5\]
Leading us to the general solutions:
\[x=15+\frac{-112n}{14}=15-8n\]
\[y=5-\frac{42n}{14}=5-3n\]
for all \(n\in\mathbb{Z}\)
\section{Rational and Irrational numbers}
A real number is called rational if it can be written in the form \(\frac{m}{n}\) for \(m,\,n\rational\). If we write \(\,q=\frac{a}{b}:\gcd(a,b)=1\) then we say \(q\) is in its lowest terms. We also have uniqueness such that there is only one way to express a rational in its lowest term.
\subsection*{Algebra of rationals}
If we take 2 integers, \(m,\,n\integer\) then we have:
\begin{itemize}
    \item \(m+n\integer\)
    \item \(m-n\integer\)
    \item \(mn\integer\)
    \item \(\frac{m}{n}\) maybe not an integer
\end{itemize}
We say \(\mathbb{Z}\) is closed under addition to mean:
\[m,\,n\integer\implies m+n\integer\]
\(\mathbb{R},\,\mathbb{Q},\,\mathbb{C}\) are all examples of what we call "fields".
\begin{table}[h]
    \begin{tabular}{c c|c|c|c}
        & Closed under "\(+\)"? & Closed under "\(-\)"? & Closed under "\(\times\)"? & Closed under "\(\div\backslash \{0\}\)"? \\
       \(\mathbb{R}\) & Yes & Yes & Yes & Yes \\
       \(\mathbb{Q}\) & Yes & Yes & Yes & Yes \\
       \(\mathbb{Z}\) & Yes & Yee & Yes & No \\
       \(\mathbb{R}\backslash\mathbb{Q}\) & No & No & No & No
    \end{tabular}
\end{table}
\subsection*{Density of the irrationals}
Between every two rational numbers there is an irrational number.
\section{Relations and Equivalence Relations}
\subsection*{Ordered pairs and Cartesian products}
We can write an ordered pair as \((a\in A,b\in B)\) where \((a,b)\ne(b,a)\) unless \(a=b\). That is to say, the order they are in matters. The Cartesian product is defined as the set of all ordered pairs of two sets \(A\) and \(B\):
\[A\times B=\{(a,b): a\in A,b\in B\}\]
We usually write \(A^n\) to denote \(\underbrace{A\times A\times\dots\times A}_{n\text{ times}}\) \(A^0=\emptyset\).
\subsection*{Relations}
Informally, a relation \(R\) on a set \(A\) is a relationship between any two elements of \(A\). if \(x\) is related to \(y\) then we write \(xRy\), otherwise we write \(x\not\mathrel{R}y\). Some examples of relations are given below:
\begin{itemize}
    \item \(\leq,<,=,\ne,>,\geq\) are all relations on \(\mathbb{R}\)
    \item \(\mid\) is a relation on \(\mathbb{N}\)
\end{itemize}
There is a link between the relations on \(A\) and on \(A^2\). We can see that any relation on \(A\) is a subset of \(A^2\). Conversely every subset \(S_R\subseteq A\times A\) gives a relation. This motivates our formal definition. If \(A\) and \(B\) are two sets then a relation \(R\) between \(A\) and \(B\) is a subset \(R\subset A\times B\). If \(A=B\) then we say \(R\) is a relation on \(A\). There are a few special relations:
\begin{itemize}
    \item If \(R\) is a relation on \(A\) then \(\not\mathrel{R}\) is also a relation known as the complement relation
    \item If \(R\) is a relation on \(A\) then \(R=A\times A\) is the trivial relation
    \item If \(R\) is a relation on \(A\) then \(R=\emptyset\) is the empty relation
\end{itemize}
\subsection*{Congruence relations}
An example of some important relations are congruence relations. If \(k\) is a positive integer then \(m\) is congruent to \(n\) modulo \(k\) if \(k\mid m-n\), usually written \(m \equiv n\mod{k}\). The congruence relation is preserved under multiplication. The congruence relation is also transitive.
\begin{itemize}
    \item if \(m\equiv n \mod{k}\) and \(a\in\mathbb{Z}\), then \(am\equiv an\mod{k}\)
    \item if \(m\equiv n \mod{k}\) and \(n\equiv 0 \mod{k}\) then \(m\equiv 0 \mod{k}\)
\end{itemize}
\subsection*{Equivalence relations}
If \(A\) is a set and \(R\subset A\times A\) is a relation on \(A\), then \(R\) is said to be:
\begin{itemize}
    \item Reflexive if \(xRx,\;\forall x\in A\)
    \item Symmetric if \(xRy\iff yRx,\;\forall x,y\in A\)
    \item Transitive if \(xRy\) and \(yRz\) implies \(xRz\)
\end{itemize}
If a relation is reflexive, symmetric and transitive then it is called an equivalence relation. The most natural examples of the equality "\(=\)".
\subsection*{Equivalence classes}
If we consider the congruence relation modulo 2 on the integers then we can see that no number can be both even and odd, so we can write the set of integers as a disjoint union:
\[\mathbb{Z}=E\sqcup O\]
These are examples of equivalence classes. More generally we say that if \(R\) is an equivalence relation on a set \(A\) and \(a\in A\), then the equivalence class of \(a\) is defined as:
\[[a]=\{b\in A:bRa\}\]
We usually say that \(a\) is a representative of the equivalence class \([a]\). We therefore have \(E=[0]\) and \(O=[1]\) so we can write \(\mathbb{Z}\) as a different disjoint union:
\[\mathbb{Z}=[0]\sqcup [1]\]
An equivalence class is usually called a congruence class if the underlying relation is a congruence. Consider equivalence classes like the following: What other number/element do I need to make the relation true? We know that the relation gives us a set of tuples, so what we are essentially saying is given one of those values in the tuple, what is the set of possible values for the other one, provided that the relation is true?
\section{Modular arithmetic}
For any positive integer \(k\), we have \(+\) and \(\times\) to be well defined operations modulo \(k\). This means that we have:
\[m+n\mod{k}\equiv (m\mod{k})+(n\mod{k})\]
\[m\times n\mod{k}\equiv (m\mod{k})\times(n\mod{k})\]
\[x\equiv y\mod{k}\implies x^n\equiv y^n\mod{k}\]
We have a special case for \(x\mod{3}\), which is a consequence of us writing the digits to a number in base \(10\equiv1\mod{3}\). This means you can sum all of the digits together and take that new number mod 3 to get the same result. This is very useful sometimes. We know that multiplication is well defined however division or cancelling is not. Cancelling is only something you can do when the modulo is prime.
\[ab\equiv ac\mod{p}\implies b\equiv c\mod{p}\text{ only when }p \text{ is prime and }p\nmid a.\]
\subsection*{Multiplicative inverses}
Note that every non-zero rational has a multiplicative inverse such that \(qq^{-1}=q^{-1}q=1\). If \(p\) is prime and \(a\not \equiv 0\), then there exists \(b\) such that \(ab\equiv1\mod{p}\). This is proved using b\'ezouts lemma.
\subsection*{The system \(\mathbb{Z}/k\mathbb{Z}\)}
We write \(\mathbb{Z}/k\mathbb{Z}\) for the integers modulo \(k\). \(+\) and \(\times\) are well defined on this set. We say that \(\mathbb{Z}/k\mathbb{Z}\) is closed under addition and multiplication. we can use this to reframe our statement on multiplicative inverses:
\[a,b\in\mathbb{Z}/p\mathbb{Z}\,:\,ab\equiv1\mod{p}\]
\subsection*{Fermat's little theorem}
Let \(p\) be some prime. Then if \(a\not\equiv0\mod{p}\) then \(a^{p-1}\equiv1\mod{p}\). The proof of this statement uses the following lemmas:
\begin{itemize}
    \item If \(p\) is prime then \(p\mid \binom{p}{k}\) for \(1\leq k\leq p-1\)
    \item If \(p\) is prime and \(n\in\mathbb{N}\) then \(n^p\equiv n\mod{p}\)
\end{itemize}
\subsection*{Chinese remainder theorem}
If we have a list of numbers \(k_1,k_2,...k_n\) such that all pairs are coprime. Then we can write a system of equations of the form:
\[x\equiv a_1\mod{k_1}\]
\[x\equiv a_2\mod{k_2}\]
\[...\]
\[x\equiv a_n\mod{k_n}\]
which has an integer solution \(0\leq x\leq k_1k_2\cdots k_n\). Any two solutions are congruent modulo \(k_1k_2\cdots k_n\).
\subsubsection*{Intuition and how to solve problems}
The main idea about the Chinese remainder theorem is to construct a number as the addition of \(n\) components, such that only one component satisfies each congruence requirement. This is also why we require all of the modulus's to be coprime, so that we do not have one as a multiple of another. when we have equations of the form:
\[x\equiv a_i\mod{k_i}\]
We can write the solution as:
\[x=k_2k_3\cdots k_n+k_1k_3\cdots k_n+k_1k_2\cdots k_n+...\]
Then we have to introduce a new factor to each addition terms such that when it is modded we get the correct \(a_i\). Call this factor \(b_i\), then:
\[x=b_1k_2k_3\cdots k_n+b_2k_1k_3\cdots k_n+b_3k_1k_2\cdots k_n+...\]
Then all equations are satisfied, but we should also realise that they are all satisfied modulus \(k_1k_2k_3\cdots k_n\). Hence our final solutions is:
\[x\equiv b_1k_2k_3\cdots k_n+b_2k_1k_3\cdots k_n+b_3k_1k_2\cdots k_n+...\mod{k_1k_2k_3\cdots k_n}\]
\section{Polynomials}
This section is very definition heavy because it is mostly working with polynomial objects that we are very familiar with already. The key ideas are as follows:
\begin{itemize}
    \item A polynomial is an expression and not a function.
    \item The highest power is called the degree of the polynomial denoted \(\deg(p)\) and its prefactor \(a_n\) is called the leading coefficient
    \item If all coefficients are 0, then we have the zero polynomial and we define \(\deg{(0)}=-\infty\)
    \item The polynomial is called "monic" if the leading coefficient is 1
    \item The set of all polynomials in a specific ring \(R\) is denoted by: \(R[x]\). Note that our definition allows for polynomials in \(\mathbb{Z}/k\mathbb{Z}\)
\end{itemize}
It is trivial to show that \(R[x]\) is closed under addition, subtraction and multiplication. It can also be shown that \(R[x]\) satisfies all of the requirements to ba a vector-space. In particular we have the set of all polynomials up to degree \(n\) to be:
\[R[x]_{\leq n}\cong R^{n+1}\]
\section{Functions}
Note that most of this content has already been covered in MATH1101 so will just be briefly discussed here, any potential differences will be highlighted.
\subsection*{Images}
If \(f:A\to B\) and \(a\in A\) then \(f(a)\in B\) is called the "image" of \(a\). If \(C\subseteq A\) then the set \(\{f(c):c\in C\}\) is called the image of \(C\). The image of the entire set \(A\) is called the range of \(f\) and written \(\text{Im}(f)\).  
\subsection*{Pre-Images}
If \(f:A\to B\) and \(b\in B\) then \(\{a\in A: f(a)=b\}\) is called the pre-image of \(b\) and denoted \(f^{-1}(b)\). If \(D\subseteq B\) then the set \(f^{-1}(D)=\{a\in A: f(a)\in D\}\) is the pre-image of \(D\). Note that in general the pre-image of an element of a set is also a set.
\subsection*{Classifications}
\begin{itemize}
    \item Injection: unique elements map to unique elements (one-to-one)
    \item Surjection: all elements map to an element (onto)
    \item Bijection: both of the above.
\end{itemize}
\subsection*{Set-valued functions}
It is important to clarify that the domain and codomains of functions need not consist of just "points" but can be made up of sets of points, or sets of functions etc. It is perfectly valid for a function to be defined as such:
\[f:\mathbb{R}\times\mathbb{R}\to\mathcal{P}(\mathbb{R})\]
\subsection*{Inverse functions}
For any set \(A\) we can define an identity function denoted \(i_A\) such that:
\[i_A:A\to A;\;i_A(x)=x\qquad\forall x\in A\]
This function can be shown to act similar to other identity functions in that composition with it does not change the function. If we have \(f:A\to B\):
\[i_B\circ f=f=f\circ i_A\]
from this definition we can define what we mean by an inverse function. If we have a bijective function \(f:A\to B\), then \(g:B\to A\) is said to be the inverse of \(f\) if:
\[f\circ g=i_B\]
\[g\circ f=i_A\]
We have that \(g\) is also bijective and is unique.
\subsection*{Functions between finite sets}
We reiterate the pigeonhole principle for completeness. If \(f:A\to B\) and \(\vert A\vert = \vert B \vert\) then:
\[f\text{ is surjective}\iff f \text{ is injective}\]
This method can also be used to prove that a solution exists in the Chinese remainder theorem, but it doesn't give you any information about that solution.
\section{Permutations}
\subsection*{Permutations on sets}
Informally we talk about permutations and just reordering of sets however we can use functions to define permutations more mathematically. A permutation of a set \(X\) is a bijection:
\[f:X\to X\]
Most of the time we only consider permutations of finite sets and in particular the finite set \(\{1,2,3,...n\}\) for \(n\geq 1\). In this case the set of all permutations is denoted \(S_n\) and we usually use Greek letters to denote elements of \(S_n\). A permutation \(\sigma\) is defined by its elements \(\sigma(1),\sigma(2),...,\sigma(n)\). We can represent these elements as a vector but it is more common to use two line notation:
\[\begin{pmatrix}
    1 & 2 & 3 & \cdots & n \\
    \sigma(1) & \sigma(2) & \sigma(3) & \cdots & \sigma(n)
\end{pmatrix}\]
The question of how many permutations there are in \(S_n\) is the same as asking how many ways we can pick \(\sigma(1),\sigma(2)\cdots\).
\subsection*{Compositions and inverses}
Since we are dealing with bijections and we know that function composition preserves bijections, we can see that if \(\sigma,\tau\in S_n\) then \(\sigma\circ\tau\) and \(\tau\circ\sigma\) are also permutations and hence in \(S_n\). This leads us to call permutations closed under composition. When dealing with compositions we often talk about multiplying or taking products of compositions and write them without the \(\circ\) symbol. We are still talking about compositions thought. For example:
\[n=3\]
\[\sigma=\begin{pmatrix}
    1 & 2 & 3 \\
    2 & 1 & 3
\end{pmatrix}\]
\[\rho=\begin{pmatrix}
    1 & 2 & 3 \\
    1 & 3 & 2
\end{pmatrix}\]
\[\rho\sigma = \rho\circ \sigma = \begin{pmatrix}
    1 & 2 & 3 \\
    \rho(\sigma(1)) & \rho(\sigma(2)) & \rho(\sigma(3))
\end{pmatrix}=\begin{pmatrix}
    1 & 2 & 3 \\
    3 & 1 & 2
\end{pmatrix}\]
We should note that function composition is not necessarily commutative but can be. 
\subsection*{Identity permutation}
The identity permutation of a set \(X\) is denoted \(i_X\) and is the result of applying the identity map \(i(x)=x\) to each element in \(X\).
\subsection*{Powers of permutations}
Given a permutation \(\sigma\) we define its powers \(\sigma^k\) for \(k= 0\) with \(\sigma^0=i\) and \(\sigma^k=\sigma^{k-1}\circ\sigma\) for \(k\geq 1\).
\subsection*{Order of a permutation}
Given a permutation \(\sigma\), we define the order of \(\sigma\) as the smallest positive integer power \(k\) such that \(\sigma^k=i\), it can be shown that this power always exists.
\subsection*{Inverses of permutations}
Since each permutation is a bijection we know that it must have an inverse such that \(\sigma\circ\sigma^{-1}=i\). These inverses obey the regular inverse rules such as:
\[(\sigma\tau)^{-1}=\tau^{-1}\sigma^{-1}\]
\subsection*{Important results}
It can be shown that the set of permutations \(S_n\) forms a group as it satisfies the following properties:
\begin{itemize}
    \item Closure: if \(\sigma,\tau\in S_n\) then \(\sigma\tau\in S_n\) 
    \item Associativity: if \(\sigma,\tau,\delta\in S_n\) then \(\sigma(\tau\delta)=(\sigma\tau)\delta\)
    \item Identity: there exists an identity element \(i\in S_n\) such that for any \(\sigma\in S_n\), \(\sigma i=i\sigma\)
    \item Inverses: for every \(\sigma\in S_n\) there exists an inverse \(\sigma^{-1}\in S_n\) such that \(\sigma\sigma^{-1}=\sigma^{-1}\sigma=i\)
\end{itemize}

\hrulefill
\begin{center}
End of Autumn content
\end{center}
\hrulefill
\section{Introduction to group theory}
\subsection*{Binary operations}
A binary operation is one such that you take two elements from an abstract set and produce a third. We can see that many of the operations we do all of the time are binary operations: consider addition of integers. Without too much thought it can be seen that addition in \(\mathbb{Z}\) is a function \(\mathbb{Z}\times\mathbb{Z}\to\mathbb{Z};\; (a,b)\mapsto a+b\). If we consider other common sets and operations we can see a clear trend that motivates our definition: Let \(A\) be a set. A binary operation in \(A\) is a function \(*:A\times A\to A;\;(a,b)\mapsto a*b\). We can encode this as \((A,*)\), where the set and operation definitions matter. Some common examples are:
\begin{itemize}
    \item Addition: \((\mathbb{Z},+),\,(\mathbb{R}[x])\), etc...
    \item Multiplication: \((\mathbb{Q},\times),\,(\mathbb{C},\times)\), etc...
    \item Composition: \((S_n,\circ)\), etc...
\end{itemize}
Our definition is very flexible and as such not all binary operations will arise naturally, although often we will construct them to mirror natural properties.
\subsection*{Cayley tables}
We cam represent binary operations in Cayley tables which can be useful for visualising the mappings and for seeing any patterns. It is also just more efficient to write for large sets. Consider the set \(A=\{1,2,3\}\) and the operation \(*\) defined by:
\begin{table}[H]
    \begin{tabular}{c|c c c}
        \(*\) & \textbf{1} & \textbf{2} & \textbf{3} \\ \hline
        \textbf{1} & \(\lambda_1\) & \(\lambda_2\) & \(\lambda_3\) \\
        \textbf{2} & \(\lambda_4\) & \(\lambda_5\) & \(\lambda_6\) \\
        \textbf{3} & \(\lambda_7\) & \(\lambda_8\) & \(\lambda_9\) \\
    \end{tabular}
\end{table}
where \(\lambda_n\in A,\,\forall\, n\leq9\).
\subsection*{The group axioms}
Let \(A\) be a set with binary operation \(*\), then:
\begin{itemize}
    \item An element \(e\in A\) is called an identity element of \(A\) if \(\forall a\in A\) we have \(e*a=a*e=a\). Note that it is important we have both sides of the equation to have an identity element.
    \item If A has an identity element \(e\) such that for any element \(a\in A\), we also have \(b\in A\) with \(a*b=b*a=e\), the element \(b\) is known as the inverse of \(a\).
    \item If the operation \(*\) is associative, then for any \(a,b,c\in A\), \(a*(b*c)=(a*b)*c=a*b*c\)
\end{itemize}
Now a group is a set \(G\) with a binary operation \(*\) such that all of the above is true. A group must satisfy the following conditions:
\begin{itemize}
    \item \(G\) has an identity element.
    \item Every element in \(G\) has a unique inverse also in \(G\).
    \item The binary operation \(*\) is associative.
\end{itemize}
We denote the group \(\langle G,*\rangle\).
\subsection*{The general linear group}
Sometimes we need to do some wrangling to get a group because we want to be able to use the properties of a group but our objects do not naturally form one. A good example of this is matrices with matrix multiplication. We know their exists matrices that do not have defined inverses and so to turn our matrices set into a group we just remove all of those. Consider the general linear group over \(\mathbb{R}\) in dimension 2:
\[\text{GL}_2(\mathbb{R})=\{A\in M_2(\mathbb{R}):\det(A)\ne0\}\]
\subsection*{An abstract cancellation theorem}
The rule power of using groups is to prove general results that hold true when applied to the specific instances that we see in everyday life. A good example of this would be cancellation. Instead of proving hundreds of different cancellation theorems we can just prove one using an abstract group.
\[g*r=h*r\]
\[g*r*r^{-1}=h*r*r^{-1}\]
\[g*(r*r^{-1})=h*(r*r^{-1})\]
\[g*e =h*e\]
\[g*h\]
\subsection*{Types of groups}
There are many types of general groups that we will come across but some of particular note are:
\begin{itemize}
    \item \(\text{GL}_2(\mathbb{R})\)
    \item Finite matrix groups with multiplication
    \item Cyclic groups such as \(\mathbb{{Z}}/k\mathbb{Z}\) with addition
    \item Product groups such as \(G\times H\) with \(\langle G,*\rangle,\  \langle H,\circ\rangle\)
\end{itemize}
Different groups can have the same Cayley table (up to relabelling) because they represent the same fundamental property or object. In this case we call the groups an isomorphism.
\subsubsection*{Product groups}
A natural binary operation on \(G\times H\) might be \(*\) with:
\[(g_1,h_1)*(g_2,h_2)=(g_1*g_2,h_1\circ h_2)\]
it can be seen that this still forms a group, and this should make sense in the context of \(\mathbb{R}^n\) it just represents performing an operation component-wise which we do all the time. In fact, generally if \(V\) is a vector space then \(\langle\mathbb{R}^n,+\rangle\) is a group.
\section{Properties of groups}
When talking about a group with a binary operation it is very standard to drop the operation symbol and just write the two elements next to each other. For example, for the group \(\langle G,*\rangle\) with \(g,h\in G\) we write \(gh\) for \(g*h\). Generally we talk about this as "multiplication" even though we could be dealing with any binary operation. The main exception to this is when we are looking at an additive group, in which case we tend to use the standard notation \(g+h\) to avoid confusion.
\subsection*{Basic properties of groups}
Groups have some key properties that come up often and should be known. If \(G\) is a group with \(*\) and elements \(g,h,r,l\in G\):
\begin{itemize}
    \item \(gr=hr\implies g=h\)
    \item \(lg=lh\implies g=h\)
    \item \(\forall g,h\in G,\,\exists!\,r\in G:gr=h\)
    \item \(\forall g,h\in G,\,\exists!\,s\in G:sg=h\)
    \item \((g^{-1})^{-1}=g\)
    \item \((gh)^{-1}=h^{-1}g^{-1}\)
\end{itemize}
\subsection*{Powers of elements}
Given a group \(\langle G,*\rangle\) and \(g\in G\) we have powers that work exactly as expected:
\[g^2=g*g\]
\[g^3=g*g*g\]
\[g^n=\underbrace{g*g*g*...*g}_{\text{n-times}}\]
\[g^{-n}=\underbrace{g^{-1}*g^{-1}*g^{-1}*...*g^{-1}}_{\text{n-times}}\]
Again, with some specific additive groups we instead write powers as \(ng\) to avoid some confusion. Laws of indices work exactly as expected:
\[g^{-n}=(g^{-1})^n\]
\[g^mg^n=g^{m+n}\]
\[(g^m)^n=g^{mn}\]
\subsection*{Order of groups and elements}
A group \(G\) is finite if it has a finite number of elements. We define ord\((G)=|G|\) sometimes written \(\#G\). We say \(G\) is infinite if it has an infinite number of elements, in this case we say ord\((G)=\infty\). Now let \(g\in G\), we define ord\((g)\) as the smallest positive integer \(n\) such that \(g^n=e\), if this number does not exist then we say ord\((g)=\infty\). Sometimes the order of an element is written o\((g)\).
\subsubsection*{Order of cyclic groups}
Consider the cyclic groups \(\mathbb{Z}/ k\mathbb{Z}\), examining the possible orders of each element we find that the possible orders are exactly the divisors of \(k\).
\subsubsection*{Divisor rules for order}
Consider an arbitrary group \(G\) with \(g\in G\) and ord\((g)=n\). Then let \(r,s,t\in \mathbb{Z}\), we have:
\[g^r=e\implies n\mid r\]
\[g^s=g^t\implies n\mid s-t\]
\subsection*{Abelian groups}
Let \(G\) be a group. We say two elements \(g,h\in G\) commute if:
\[g*h=h*g\]
We say a group \(G\) is Abelian if all elements commute, that is \(\forall g,h\in G\) we have \(gh=hg\). \(S_3\) is the simplest example of a non-abelian group which makes it very useful as a counter example in proofs. It is important to look for counter-examples first when asked to disprove something.
\section{Subgroups}
Let \(G\) be a group. A subgroup of \(G\) is a non-empty subset \(H\subset G\) such that:
\begin{itemize}
    \item If \(g,h\in H\) then \(g*h\in H\) - We have closure under operation.
    \item If \(g\in H\) then \(g^{-1}\in H\) - We have closure under inverses.
\end{itemize}
In this case, we write \(H\leqslant G\). As a lemma to this definition, if \(\langle G,*\rangle\) is a group and \(H\leqslant G\) is a subgroup then \(\langle H,*\rangle\) is a group.
\subsection*{Examples of subgroups}
If \(G\) is a group with identity element \(e\) then we have the trivial subgroups \(G\leqslant G\) and \(\{e\}\leqslant G\). A more interesting example might be when we have \(\mathbb{Z},\mathbb{Q},\mathbb{R},\mathbb{R}[x]\) all with addition, then:
\[\mathbb{Z}\leqslant\mathbb{Q}\leqslant\mathbb{R}\leqslant\mathbb{R}[x]\]
Another interesting subgroup is the special linear group:
\[\text{SL}_2(\mathbb{R})=\{A\in \text{GL}_2(\mathbb{R}):\det(A)=1\}\]
\subsection*{Cyclic subgroups}
Let \(G\) be a group and \(g\in G\). The cyclic subgroup generated by \(g\) is given by:
\[\langle g\rangle=\{g^r:r\in\mathbb{Z}\}\]
Intuitively this is the set of values we get when if we keep applying the binary operation on \(g\) over and over. When dealing with addition as a binary operation we tend to use some different notation: Let \(G=\langle\mathbb{Z},+\rangle\) and \(m\in\mathbb{Z}\) then:
\[\langle m\rangle =m\mathbb{Z}=\{\dots,-2m,-m,0,m,2m,\dots\}\]
We say that \(G\) is cyclic if \(G=\langle g\rangle\) for some \(g\in G\) and in this case we describe \(g\) as a "generator" of \(G\).
\subsection*{Generators of groups}
Cyclic groups are generated by 1 element but we can also talk about groups or subgroups being generated by more elements. For example, the subgroup of \(\langle g_1,g_2\rangle\) is the subset of all combinations of \(g_1\) and \(g_2\):
\[g_1,\,g_2,\,g_1g_2,\,\dots\,g_1^7g_2^2g_1,\,\dots\]
We say the group \(G\) is generated by \(g_1,g_2,\dots, g_r\) if \(G=\langle g_1,g_2,\dots, g_r\rangle\).
\section{Symmetry groups}
\end{document}
